{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Callable\n",
    "import abc\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    @abc.abstractmethod\n",
    "    def optimize(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescentOptimizer(Optimizer):\n",
    "    def __init__(self, learning_rate: float = 0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def optimize(self, val, grad):\n",
    "        return val - self.learning_rate * grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunction(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def __call__(self, x):\n",
    "        pass\n",
    "    @abc.abstractmethod\n",
    "    def gradient(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(ActivationFunction):\n",
    "    def __call__(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def gradient(self, x):\n",
    "        sig = self.__call__(x)\n",
    "        return sig * (1 - sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(ActivationFunction):\n",
    "    def __call__(self, x):\n",
    "        return np.where(x >= 0, x, 0)\n",
    "    \n",
    "    def gradient(self, x):\n",
    "        return np.where(x >= 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Initializer(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def __call__(self, var):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniformInitializer(Initializer):\n",
    "    def __call__(self, mean, var, shape):\n",
    "        stddev = np.sqrt(var)\n",
    "        lim = np.sqrt(3) * stddev\n",
    "        \n",
    "        return np.random.uniform(-lim, lim, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def set_optimizer(self, optimizer):\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def forward_pass(self, X):\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def backward_pass(self, X):\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def output_shape(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, n_units, input_shape):\n",
    "        self.n_units = n_units\n",
    "        \n",
    "        if len(input_shape) != 2:\n",
    "            raise Exception('Input shape other than 2 not supported.')\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "    \n",
    "    def output_shape(self):\n",
    "        return (self.input_shape[0], self.n_units)\n",
    "    \n",
    "    def set_activation(self, activation):\n",
    "        if not isinstance(activation, Activation):\n",
    "            raise Exception('The activation object provided is not an instance of Activation class.')\n",
    "        \n",
    "        self.activation = activation\n",
    "    \n",
    "    def initialize(self, initializer):\n",
    "        if not isinstance(initializer, Initializer):\n",
    "            raise Exception('The initializer object provided is not an instance of Initializer class.')\n",
    "        \n",
    "        self.initializer = initializer\n",
    "        \n",
    "        self.W = self.initializer(mean=0, var=1/self.n_units, shape=(self.input_shape[1], self.n_units))\n",
    "        self.b = self.initializer(mean=0, var=1/self.n_units, shape=(1, self.n_units))\n",
    "    \n",
    "    def set_optimizer(self, optimizer):\n",
    "        \n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise Exception('The optimizer object provided is not an instance of Optimizer class.')\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        return X.dot(self.W) + self.b\n",
    "    \n",
    "    def backward_pass(self, cum_grad, X):\n",
    "        grad_W = X.T.dot(cum_grad)\n",
    "        grad_b = np.sum(cum_grad, axis=0)\n",
    "        \n",
    "        self.W = self.optimizer.optimize(self.W, grad_W)\n",
    "        self.b = self.optimizer.optimize(self.b, grad_b)\n",
    "        \n",
    "        return cum_grad.dot(self.W.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layer = Dense(n_units=5, input_shape=(3, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_layer.output_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layer.initialize(UniformInitializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layer.set_optimizer(GradientDescentOptimizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.28790194, -0.56390246, -0.12119454, -0.65586433,  0.5268497 ],\n",
       "       [ 0.10501284, -0.02428236,  0.10796078,  0.48601642, -0.3941557 ],\n",
       "       [-0.64008093, -0.44022788, -0.49671487,  0.35091914,  0.35538506],\n",
       "       [-0.29073183, -0.48654303,  0.11626326,  0.06656025, -0.14395966],\n",
       "       [-0.44386857, -0.46222169, -0.75541802,  0.27534252,  0.18249313],\n",
       "       [ 0.06081766,  0.70426789, -0.19652398, -0.03197299,  0.27281891],\n",
       "       [-0.08871465,  0.02247918, -0.36850942, -0.73376416,  0.04873917],\n",
       "       [-0.43981468,  0.75404555, -0.66764092,  0.40116832,  0.65169349],\n",
       "       [-0.52365192, -0.47923632, -0.49422007,  0.64281943,  0.71394737],\n",
       "       [-0.74755722,  0.61740316,  0.25870522, -0.5180662 , -0.13991368],\n",
       "       [ 0.02254485, -0.47263025,  0.22032634,  0.05873256, -0.2216663 ],\n",
       "       [ 0.72590164, -0.29757616,  0.05550272,  0.22078828,  0.35942142]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_layer.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.67620969,  0.13055199, -1.02071947, -0.888411  , -1.35367263,\n",
       "         0.65940749, -1.26976986,  0.54945177, -0.29034151, -0.67942871,\n",
       "        -0.5426928 ,  0.9140379 ],\n",
       "       [-0.67620969,  0.13055199, -1.02071947, -0.888411  , -1.35367263,\n",
       "         0.65940749, -1.26976986,  0.54945177, -0.29034151, -0.67942871,\n",
       "        -0.5426928 ,  0.9140379 ],\n",
       "       [-0.67620969,  0.13055199, -1.02071947, -0.888411  , -1.35367263,\n",
       "         0.65940749, -1.26976986,  0.54945177, -0.29034151, -0.67942871,\n",
       "        -0.5426928 ,  0.9140379 ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_layer.backward_pass(cum_grad=np.ones((3, 5)), X=np.ones((3, 12)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.25790194, -0.59390246, -0.15119454, -0.68586433,  0.4968497 ],\n",
       "       [ 0.07501284, -0.05428236,  0.07796078,  0.45601642, -0.4241557 ],\n",
       "       [-0.67008093, -0.47022788, -0.52671487,  0.32091914,  0.32538506],\n",
       "       [-0.32073183, -0.51654303,  0.08626326,  0.03656025, -0.17395966],\n",
       "       [-0.47386857, -0.49222169, -0.78541802,  0.24534252,  0.15249313],\n",
       "       [ 0.03081766,  0.67426789, -0.22652398, -0.06197299,  0.24281891],\n",
       "       [-0.11871465, -0.00752082, -0.39850942, -0.76376416,  0.01873917],\n",
       "       [-0.46981468,  0.72404555, -0.69764092,  0.37116832,  0.62169349],\n",
       "       [-0.55365192, -0.50923632, -0.52422007,  0.61281943,  0.68394737],\n",
       "       [-0.77755722,  0.58740316,  0.22870522, -0.5480662 , -0.16991368],\n",
       "       [-0.00745515, -0.50263025,  0.19032634,  0.02873256, -0.2516663 ],\n",
       "       [ 0.69590164, -0.32757616,  0.02550272,  0.19078828,  0.32942142]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_layer.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def __call__(self):\n",
    "        pass\n",
    "    @abc.abstractmethod\n",
    "    def gradient(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryCrossEntropyLoss(LossFunction):\n",
    "    def __call__(self, y, y_pred):\n",
    "        return -np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred), axis=0)\n",
    "    \n",
    "    def gradient(self, y, y_pred):\n",
    "        return (y_pred - y) / (y_pred - y_pred * y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.42144206])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bce_loss = BinaryCrossEntropyLoss()\n",
    "bce_loss(np.array([[1, 0, 0, 0]]).T, np.array([[0.9, 0.1, 0.1, 0.1]]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.21034037])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bce_loss(np.array([[1, 0, 0, 0]]).T, np.array([[0.1, 0.9, 0.9, 0.9]]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -1.01010101],\n",
       "       [  1.01010101],\n",
       "       [100.        ],\n",
       "       [ 10.        ]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bce_loss.gradient(np.array([[1, 0, 0, 0]]).T, np.array([[0.99, 0.01, 0.99, 0.9]]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def __init__(self, components=None):\n",
    "        pass\n",
    "    @abc.abstractmethod\n",
    "    def set_optimizer(self, optimizer):\n",
    "        pass\n",
    "    @abc.abstractmethod\n",
    "    def set_loss_function(self, loss_function):\n",
    "        pass\n",
    "    @abc.abstractmethod\n",
    "    def predict_proba(self, X):\n",
    "        pass\n",
    "    @abc.abstractmethod\n",
    "    def predict(self, X, thresh=None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Model):\n",
    "    def __init__(self, components=None):\n",
    "        self.components = []\n",
    "        self.loss_function = None\n",
    "        \n",
    "        for component in components:\n",
    "            self.components.append(component)\n",
    "    \n",
    "    def set_optimizer(self, optimizer):\n",
    "        for component in self.components:\n",
    "            if isinstance(component, Layer):\n",
    "                component.set_optimizer(optimizer)\n",
    "    \n",
    "    def set_loss_function(self, loss_function):\n",
    "        self.loss_function = loss_function\n",
    "    \n",
    "    def initialize(self, initializer):\n",
    "        \n",
    "        for component in self.components:\n",
    "            if isinstance(component, Layer):\n",
    "                curr = component.initialize(initializer)\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        cache = []\n",
    "        curr = X\n",
    "        \n",
    "        for component in self.components:\n",
    "            cache.append(curr)\n",
    "            if isinstance(component, Layer):\n",
    "                curr = component.forward_pass(curr)\n",
    "            else:\n",
    "                curr = component(curr)\n",
    "        \n",
    "        return curr, cache\n",
    "    \n",
    "    def backward_pass(self, loss_grad, cache):\n",
    "        cum_grad = loss_grad\n",
    "        \n",
    "        for component, inp in zip(reversed(self.components), reversed(cache)):\n",
    "            if isinstance(component, Layer):\n",
    "                cum_grad = component.backward_pass(cum_grad, inp)\n",
    "            else:\n",
    "                cum_grad = component.gradient(inp) * cum_grad\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        y_pred, _ = self.forward_pass(X)\n",
    "        return y_pred\n",
    "    \n",
    "    def predict(self, X, thresh=.5):\n",
    "        y_pred = self.predict_proba(X)\n",
    "        return np.where(y_pred > thresh, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([Dense(n_units=10, input_shape=(150, 4)),\n",
    "                    ReLU(),\n",
    "                    Dense(n_units=1, input_shape=(10, 10)),\n",
    "                    Sigmoid()])\n",
    "model.initialize(UniformInitializer())\n",
    "model.set_loss_function(BinaryCrossEntropyLoss())\n",
    "model.set_optimizer(GradientDescentOptimizer(learning_rate=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data = load_iris()\n",
    "X = iris_data['data'] # Features\n",
    "y = np.where(iris_data['target'] < 0.5, 0, 1) # Target classes\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At iteration 0 the loss is 181.84640475789402\n",
      "At iteration 1 the loss is 99.35290507937226\n",
      "At iteration 2 the loss is 67.36270293863662\n",
      "At iteration 3 the loss is 45.74474588693522\n",
      "At iteration 4 the loss is 34.986572546050915\n",
      "At iteration 5 the loss is 29.21473592990995\n",
      "At iteration 6 the loss is 24.822994649684855\n",
      "At iteration 7 the loss is 20.88756961130935\n",
      "At iteration 8 the loss is 17.457589417148807\n",
      "At iteration 9 the loss is 14.582764586563307\n",
      "At iteration 10 the loss is 12.248199507501996\n",
      "At iteration 11 the loss is 10.383144817643197\n",
      "At iteration 12 the loss is 8.899932158553296\n",
      "At iteration 13 the loss is 7.795116754852851\n",
      "At iteration 14 the loss is 6.983365292033278\n",
      "At iteration 15 the loss is 6.332974122305083\n",
      "At iteration 16 the loss is 5.831318181855369\n",
      "At iteration 17 the loss is 5.436541202091599\n",
      "At iteration 18 the loss is 5.11194165657544\n",
      "At iteration 19 the loss is 4.832185504643837\n",
      "At iteration 20 the loss is 4.585929082021753\n",
      "At iteration 21 the loss is 4.374286546765348\n",
      "At iteration 22 the loss is 4.193616474417589\n",
      "At iteration 23 the loss is 4.033022737840473\n",
      "At iteration 24 the loss is 3.887175343093789\n",
      "At iteration 25 the loss is 3.7565290815420673\n",
      "At iteration 26 the loss is 3.637424268176398\n",
      "At iteration 27 the loss is 3.527095760318368\n",
      "At iteration 28 the loss is 3.424486952182863\n",
      "At iteration 29 the loss is 3.328404949197975\n",
      "At iteration 30 the loss is 3.2385470904179408\n",
      "At iteration 31 the loss is 3.1539217147050573\n",
      "At iteration 32 the loss is 3.0737334735469246\n",
      "At iteration 33 the loss is 2.997727215372498\n",
      "At iteration 34 the loss is 2.9257871109151448\n",
      "At iteration 35 the loss is 2.8573629868988504\n",
      "At iteration 36 the loss is 2.792122261896841\n",
      "At iteration 37 the loss is 2.729698895221861\n",
      "At iteration 38 the loss is 2.670103172667104\n",
      "At iteration 39 the loss is 2.6130860662094455\n",
      "At iteration 40 the loss is 2.5584701540330115\n",
      "At iteration 41 the loss is 2.50614867623638\n",
      "At iteration 42 the loss is 2.455903284764849\n",
      "At iteration 43 the loss is 2.4075391478136594\n",
      "At iteration 44 the loss is 2.360942716477554\n",
      "At iteration 45 the loss is 2.3160405575723164\n",
      "At iteration 46 the loss is 2.2727739191112515\n",
      "At iteration 47 the loss is 2.231003579482393\n",
      "At iteration 48 the loss is 2.1906508923533203\n",
      "At iteration 49 the loss is 2.151663496147505\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train[:, np.newaxis]\n",
    "\n",
    "for i in range(50):\n",
    "    y_pred, cache = model.forward_pass(X_train)\n",
    "    loss = model.loss_function(y_train, y_pred)\n",
    "    print('At iteration', i, 'the loss is', loss[0])\n",
    "    grad = model.loss_function.gradient(y_train, y_pred)\n",
    "    model.backward_pass(grad, cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
